{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9857a203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\Projects\\Facepoint Recognizer\\Face-Orientation-Detector\\faceenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# BLOCK A: imports, paths, device\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ultralytics import YOLO  # YOLOv11 via ultralytics\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Paths (adjust DATA_ROOT if needed)\n",
    "DATA_ROOT = Path(r\"D:/Work/Projects/Facepoint Recognizer/Face-Orientation-Detector/data\")   # <-- change if your root is different\n",
    "TRAIN_DIR = DATA_ROOT / \"train\"\n",
    "TEST_DIR  = DATA_ROOT / \"test\"\n",
    "\n",
    "COCO_TRAIN_JSON = TRAIN_DIR / \"_annotations.coco.json\"\n",
    "COCO_TEST_JSON  = TEST_DIR  / \"_annotations.coco.json\"\n",
    "\n",
    "# Processed output\n",
    "PROC_ROOT = DATA_ROOT / \"processed_yolo\"\n",
    "PROC_TRAIN_DIR = PROC_ROOT / \"train\"\n",
    "PROC_VAL_DIR   = PROC_ROOT / \"val\"\n",
    "\n",
    "PROC_TRAIN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROC_VAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af3cb3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 'head' category with 26 keypoints.\n",
      "Loaded 515 head annotations\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>bbox</th>\n",
       "      <th>keypoints_x</th>\n",
       "      <th>keypoints_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>image_f1f0e67c_jpg.rf.d3a8d0305ac8db19cb9e1eb0...</td>\n",
       "      <td>1280</td>\n",
       "      <td>1280</td>\n",
       "      <td>[0, 0, 1212.475, 793.518]</td>\n",
       "      <td>[1093.003, 1080.399, 1163.084, 1120.685, 1135....</td>\n",
       "      <td>[298.578, 359.332, 496.351, 558.768, 574.951, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>image_9ddc2611_jpg.rf.c3c6a6e39aff4c63cec65c7d...</td>\n",
       "      <td>1280</td>\n",
       "      <td>1280</td>\n",
       "      <td>[0, 0, 961.884, 733.776]</td>\n",
       "      <td>[852.885, 846.384, 890.255, 857.758, 866.966, ...</td>\n",
       "      <td>[318.805, 369.856, 478.678, 504.124, 519.825, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>image_b2315e84_jpg.rf.a7e085afb96272ebbd3dd489...</td>\n",
       "      <td>1280</td>\n",
       "      <td>1280</td>\n",
       "      <td>[2, 0, 735, 783.333]</td>\n",
       "      <td>[674.483, 648.902, 686.697, 623.481, 624.131, ...</td>\n",
       "      <td>[346.61, 390.304, 544.489, 569.404, 604.225, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>image_b058cc7d_jpg.rf.d73d0cbad2755b76d304d814...</td>\n",
       "      <td>1280</td>\n",
       "      <td>1280</td>\n",
       "      <td>[0, 2, 1228.773, 1018.869]</td>\n",
       "      <td>[936.555, 953.546, 1097.31, 1039.868, 1056.728...</td>\n",
       "      <td>[375.658, 489.655, 619.883, 713.567, 739.78, 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>image_fb8775d0_jpg.rf.010835876f3589c0aded7a4b...</td>\n",
       "      <td>1280</td>\n",
       "      <td>1280</td>\n",
       "      <td>[0, 0, 1089.357, 1280]</td>\n",
       "      <td>[836.936, 827.941, 977.961, 901.131, 902.892, ...</td>\n",
       "      <td>[429.582, 565.491, 798.858, 893.165, 958.754, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id                                          file_name  width  height  \\\n",
       "0         0  image_f1f0e67c_jpg.rf.d3a8d0305ac8db19cb9e1eb0...   1280    1280   \n",
       "1         1  image_9ddc2611_jpg.rf.c3c6a6e39aff4c63cec65c7d...   1280    1280   \n",
       "2         2  image_b2315e84_jpg.rf.a7e085afb96272ebbd3dd489...   1280    1280   \n",
       "3         3  image_b058cc7d_jpg.rf.d73d0cbad2755b76d304d814...   1280    1280   \n",
       "4         4  image_fb8775d0_jpg.rf.010835876f3589c0aded7a4b...   1280    1280   \n",
       "\n",
       "                         bbox  \\\n",
       "0   [0, 0, 1212.475, 793.518]   \n",
       "1    [0, 0, 961.884, 733.776]   \n",
       "2        [2, 0, 735, 783.333]   \n",
       "3  [0, 2, 1228.773, 1018.869]   \n",
       "4      [0, 0, 1089.357, 1280]   \n",
       "\n",
       "                                         keypoints_x  \\\n",
       "0  [1093.003, 1080.399, 1163.084, 1120.685, 1135....   \n",
       "1  [852.885, 846.384, 890.255, 857.758, 866.966, ...   \n",
       "2  [674.483, 648.902, 686.697, 623.481, 624.131, ...   \n",
       "3  [936.555, 953.546, 1097.31, 1039.868, 1056.728...   \n",
       "4  [836.936, 827.941, 977.961, 901.131, 902.892, ...   \n",
       "\n",
       "                                         keypoints_y  \n",
       "0  [298.578, 359.332, 496.351, 558.768, 574.951, ...  \n",
       "1  [318.805, 369.856, 478.678, 504.124, 519.825, ...  \n",
       "2  [346.61, 390.304, 544.489, 569.404, 604.225, 6...  \n",
       "3  [375.658, 489.655, 619.883, 713.567, 739.78, 8...  \n",
       "4  [429.582, 565.491, 798.858, 893.165, 958.754, ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BLOCK B: load COCO annotations into a table (for train)\n",
    "\n",
    "def load_coco_annotations(coco_path: Path):\n",
    "    with open(coco_path, \"r\") as f:\n",
    "        coco = json.load(f)\n",
    "\n",
    "    images = {img[\"id\"]: img for img in coco[\"images\"]}\n",
    "    anns = coco[\"annotations\"]\n",
    "    categories = {cat[\"id\"]: cat for cat in coco[\"categories\"]}\n",
    "\n",
    "    # find the category id for \"head\" (has keypoints)\n",
    "    head_cat_id = None\n",
    "    for cid, cat in categories.items():\n",
    "        if cat[\"name\"] == \"head\":\n",
    "            head_cat_id = cid\n",
    "            head_category = cat\n",
    "            break\n",
    "    if head_cat_id is None:\n",
    "        raise ValueError(\"Could not find 'head' category in COCO.\")\n",
    "\n",
    "    num_keypoints = len(head_category[\"keypoints\"])\n",
    "    print(f\"Found 'head' category with {num_keypoints} keypoints.\")\n",
    "\n",
    "    rows = []\n",
    "    for ann in anns:\n",
    "        if ann[\"category_id\"] != head_cat_id:\n",
    "            continue\n",
    "\n",
    "        img_info = images[ann[\"image_id\"]]\n",
    "        file_name = img_info[\"file_name\"]\n",
    "        w, h = img_info[\"width\"], img_info[\"height\"]\n",
    "\n",
    "        # COCO keypoints: [x1,y1,v1, x2,y2,v2, ...]\n",
    "        kpts_raw = ann[\"keypoints\"]\n",
    "        if len(kpts_raw) != num_keypoints * 3:\n",
    "            # Skip broken annotation\n",
    "            continue\n",
    "\n",
    "        xs, ys = [], []\n",
    "        for i in range(num_keypoints):\n",
    "            x = kpts_raw[3*i]\n",
    "            y = kpts_raw[3*i + 1]\n",
    "            v = kpts_raw[3*i + 2]  # visibility\n",
    "            if v > 0:\n",
    "                xs.append(x)\n",
    "                ys.append(y)\n",
    "            else:\n",
    "                xs.append(0.0)\n",
    "                ys.append(0.0)\n",
    "\n",
    "        bbox = ann[\"bbox\"]  # [x, y, w, h] in original image\n",
    "\n",
    "        rows.append({\n",
    "            \"image_id\": img_info[\"id\"],\n",
    "            \"file_name\": file_name,\n",
    "            \"width\": w,\n",
    "            \"height\": h,\n",
    "            \"bbox\": bbox,\n",
    "            \"keypoints_x\": xs,\n",
    "            \"keypoints_y\": ys\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    print(\"Loaded\", len(df), \"head annotations\")\n",
    "    return df, head_category\n",
    "\n",
    "train_df, head_category = load_coco_annotations(COCO_TRAIN_JSON)\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7ca84f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 412\n",
      "Val samples  : 103\n"
     ]
    }
   ],
   "source": [
    "# BLOCK C: train/val split on original train set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df_split, val_df_split = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    shuffle=True,\n",
    "    stratify=None  # dataset probably small, so no stratify\n",
    ")\n",
    "\n",
    "print(\"Train samples:\", len(train_df_split))\n",
    "print(\"Val samples  :\", len(val_df_split))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "506f2a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocess train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 412/412 [00:19<00:00, 20.99it/s]\n",
      "Preprocess val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:04<00:00, 22.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed train: 412 images\n",
      "Processed val  : 103 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# BLOCK D: YOLOv11 Option A preprocessing (run once)\n",
    "\n",
    "IMG_SIZE = 512\n",
    "\n",
    "# Load YOLOv11 detection model (small, for speed)\n",
    "yolo_model = YOLO(\"yolo11n.pt\")  # make sure weights downloaded once\n",
    "\n",
    "# class id for \"person\" in COCO is 0 for standard YOLO models\n",
    "PERSON_CLASS_ID = 0\n",
    "\n",
    "def letterbox_pad_to_square(img, keypoints_xy):\n",
    "    \"\"\"\n",
    "    img: HxWx3 (uint8)\n",
    "    keypoints_xy: list of (x, y) in original image coords\n",
    "    Returns:\n",
    "        img_512: 512x512x3\n",
    "        remapped_kpts: list of (x', y') in 512x512 coords\n",
    "    \"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    scale = IMG_SIZE / max(h, w)\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "\n",
    "    # Resize\n",
    "    resized = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # Pad to 512x512 (black)\n",
    "    pad_x = (IMG_SIZE - new_w) // 2\n",
    "    pad_y = (IMG_SIZE - new_h) // 2\n",
    "\n",
    "    img_512 = np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8)\n",
    "    img_512[pad_y:pad_y+new_h, pad_x:pad_x+new_w] = resized\n",
    "\n",
    "    # Remap keypoints\n",
    "    remapped = []\n",
    "    for (x, y) in keypoints_xy:\n",
    "        if x == 0 and y == 0:\n",
    "            remapped.append((0.0, 0.0))\n",
    "        else:\n",
    "            x_res = x * scale + pad_x\n",
    "            y_res = y * scale + pad_y\n",
    "            remapped.append((x_res, y_res))\n",
    "\n",
    "    return img_512, remapped\n",
    "\n",
    "\n",
    "def apply_yolo_mask(img_bgr, bbox_from_coco=None):\n",
    "    \"\"\"\n",
    "    Apply YOLOv11 detection, keep pixels inside person bbox, background black.\n",
    "    If YOLO fails, fallback to bbox_from_coco.\n",
    "    \"\"\"\n",
    "    h, w = img_bgr.shape[:2]\n",
    "\n",
    "    # YOLO expects RGB\n",
    "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    results = yolo_model(img_rgb, verbose=False)[0]\n",
    "\n",
    "    x1, y1, x2, y2 = None, None, None, None\n",
    "\n",
    "    if results.boxes is not None and len(results.boxes) > 0:\n",
    "        boxes = results.boxes.xyxy.cpu().numpy()\n",
    "        classes = results.boxes.cls.cpu().numpy()\n",
    "        confs = results.boxes.conf.cpu().numpy()\n",
    "\n",
    "        # filter person class\n",
    "        person_indices = [i for i, c in enumerate(classes) if int(c) == PERSON_CLASS_ID]\n",
    "        if len(person_indices) > 0:\n",
    "            # take highest confidence person\n",
    "            best_i = sorted(person_indices, key=lambda i: confs[i], reverse=True)[0]\n",
    "            x1, y1, x2, y2 = boxes[best_i]\n",
    "            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "\n",
    "    # Fallback: use COCO bbox (x,y,w,h)\n",
    "    if x1 is None and bbox_from_coco is not None:\n",
    "        bx, by, bw, bh = bbox_from_coco\n",
    "        x1, y1, x2, y2 = int(bx), int(by), int(bx + bw), int(by + bh)\n",
    "\n",
    "    # If still None, return original (no mask)\n",
    "    if x1 is None:\n",
    "        return img_bgr\n",
    "\n",
    "    mask = np.zeros((h, w), dtype=np.uint8)\n",
    "    mask[max(0, y1):min(h, y2), max(0, x1):min(w, x2)] = 1\n",
    "\n",
    "    out = img_bgr.copy()\n",
    "    out[mask == 0] = 0  # background to black\n",
    "    return out\n",
    "\n",
    "\n",
    "def preprocess_split(df_split, split_name, out_dir):\n",
    "    \"\"\"\n",
    "    df_split: train_df_split or val_df_split subset\n",
    "    split_name: \"train\" or \"val\"\n",
    "    out_dir: directory to save images\n",
    "    Returns: DataFrame with new info: file_name_512, keypoints_x_512, keypoints_y_512\n",
    "    \"\"\"\n",
    "    rows_out = []\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for idx, row in tqdm(df_split.iterrows(), total=len(df_split), desc=f\"Preprocess {split_name}\"):\n",
    "        img_path = TRAIN_DIR / row[\"file_name\"]  # original images are under train/\n",
    "        if not img_path.exists():\n",
    "            print(\"Missing image:\", img_path)\n",
    "            continue\n",
    "\n",
    "        img_bgr = cv2.imread(str(img_path))\n",
    "        if img_bgr is None:\n",
    "            print(\"Failed to read:\", img_path)\n",
    "            continue\n",
    "\n",
    "        # 1) YOLO mask\n",
    "        masked = apply_yolo_mask(img_bgr, bbox_from_coco=row[\"bbox\"])\n",
    "\n",
    "        # 2) letterbox to 512x512, remap kpts\n",
    "        keypoints_xy = list(zip(row[\"keypoints_x\"], row[\"keypoints_y\"]))\n",
    "        img_512, kpts_512 = letterbox_pad_to_square(masked, keypoints_xy)\n",
    "\n",
    "        # 3) save new image\n",
    "        new_fname = f\"{split_name}_{row['image_id']}.png\"\n",
    "        save_path = out_dir / new_fname\n",
    "        cv2.imwrite(str(save_path), img_512)\n",
    "\n",
    "        xs_512 = [kp[0] for kp in kpts_512]\n",
    "        ys_512 = [kp[1] for kp in kpts_512]\n",
    "\n",
    "        rows_out.append({\n",
    "            \"orig_file_name\": row[\"file_name\"],\n",
    "            \"file_name_512\": new_fname,\n",
    "            \"width_512\": IMG_SIZE,\n",
    "            \"height_512\": IMG_SIZE,\n",
    "            \"keypoints_x_512\": xs_512,\n",
    "            \"keypoints_y_512\": ys_512\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows_out)\n",
    "\n",
    "\n",
    "proc_train_df = preprocess_split(train_df_split, \"train\", PROC_TRAIN_DIR)\n",
    "proc_val_df   = preprocess_split(val_df_split, \"val\", PROC_VAL_DIR)\n",
    "\n",
    "print(\"Processed train:\", len(proc_train_df), \"images\")\n",
    "print(\"Processed val  :\", len(proc_val_df), \"images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e11e628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) save preprocessed annotations\n",
    "proc_train_df.to_pickle(PROC_ROOT / \"proc_train_df.pkl\")\n",
    "proc_val_df.to_pickle(PROC_ROOT / \"proc_val_df.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dd24e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 412\n",
      "Val   dataset size: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\Projects\\Facepoint Recognizer\\Face-Orientation-Detector\\faceenv\\Lib\\site-packages\\albumentations\\core\\validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "C:\\Users\\Aditya_Mishra\\AppData\\Local\\Temp\\ipykernel_25676\\2690699511.py:12: UserWarning: Argument(s) 'value' are not valid for transform ShiftScaleRotate\n",
      "  A.ShiftScaleRotate(\n",
      "C:\\Users\\Aditya_Mishra\\AppData\\Local\\Temp\\ipykernel_25676\\2690699511.py:23: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
      "  A.GaussNoise(var_limit=(5.0, 20.0), p=0.4),\n",
      "d:\\Work\\Projects\\Facepoint Recognizer\\Face-Orientation-Detector\\faceenv\\Lib\\site-packages\\albumentations\\core\\composition.py:331: UserWarning: Got processor for keypoints, but no transform to process it.\n",
      "  self._set_keys()\n"
     ]
    }
   ],
   "source": [
    "# BLOCK E: Albumentations transforms and Dataset class\n",
    "\n",
    "NUM_KEYPOINTS = len(head_category[\"keypoints\"])  # 26 in your COCO\n",
    "IMG_SIZE = 512  # keep constant\n",
    "\n",
    "mean = (0.5, 0.5, 0.5)\n",
    "std  = (0.5, 0.5, 0.5)\n",
    "\n",
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.ShiftScaleRotate(\n",
    "            shift_limit=0.05,\n",
    "            scale_limit=0.10,\n",
    "            rotate_limit=20,\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "            value=(0, 0, 0),\n",
    "            p=0.8\n",
    "        ),\n",
    "        A.RandomBrightnessContrast(\n",
    "            brightness_limit=0.2, contrast_limit=0.2, p=0.7\n",
    "        ),\n",
    "        A.GaussNoise(var_limit=(5.0, 20.0), p=0.4),\n",
    "        A.MotionBlur(blur_limit=5, p=0.3),\n",
    "        A.Normalize(mean=mean, std=std),\n",
    "        ToTensorV2()\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format=\"xy\", remove_invisible=False)\n",
    ")\n",
    "\n",
    "val_transform = A.Compose(\n",
    "    [\n",
    "        A.Normalize(mean=mean, std=std),\n",
    "        ToTensorV2()\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format=\"xy\", remove_invisible=False)\n",
    ")\n",
    "\n",
    "\n",
    "class FaceKptsDataset(Dataset):\n",
    "    def __init__(self, df, root_dir, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = self.root_dir / row[\"file_name_512\"]\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        xs = row[\"keypoints_x_512\"]\n",
    "        ys = row[\"keypoints_y_512\"]\n",
    "        keypoints = [(float(x), float(y)) for x, y in zip(xs, ys)]\n",
    "\n",
    "        # Albumentations\n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(\n",
    "                image=img,\n",
    "                keypoints=keypoints\n",
    "            )\n",
    "            img_t = transformed[\"image\"]\n",
    "            kpts_t = transformed[\"keypoints\"]\n",
    "        else:\n",
    "            img_t = img\n",
    "            kpts_t = keypoints\n",
    "\n",
    "        # convert to tensor shape (52,)\n",
    "        kpts_arr = np.array(kpts_t, dtype=np.float32)  # (K, 2)\n",
    "        # normalise [0, IMG_SIZE] -> [0,1]\n",
    "        kpts_arr /= IMG_SIZE\n",
    "\n",
    "        kpts_flat = torch.from_numpy(kpts_arr.reshape(-1))  # (2*K,)\n",
    "\n",
    "        sample = {\n",
    "            \"image\": img_t,          # (3,H,W)\n",
    "            \"keypoints\": kpts_flat,  # (2*K,)\n",
    "            \"file_name\": row[\"file_name_512\"]\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "\n",
    "train_dataset = FaceKptsDataset(proc_train_df, PROC_TRAIN_DIR, transform=train_transform)\n",
    "val_dataset   = FaceKptsDataset(proc_val_df,   PROC_VAL_DIR,   transform=val_transform)\n",
    "\n",
    "print(\"Train dataset size:\", len(train_dataset))\n",
    "print(\"Val   dataset size:\", len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "022b0de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders ready.\n"
     ]
    }
   ],
   "source": [
    "# BLOCK F: DataLoaders\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "print(\"DataLoaders ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5ded901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNetKpts(\n",
      "  (backbone): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Identity()\n",
      "  )\n",
      "  (head): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=52, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# BLOCK G: model definition with cosine-aux loss\n",
    "\n",
    "class ResNetKpts(nn.Module):\n",
    "    def __init__(self, num_keypoints):\n",
    "        super().__init__()\n",
    "        self.backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        in_feats = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(in_feats, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, num_keypoints * 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.backbone(x)\n",
    "        out = self.head(feat)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = ResNetKpts(NUM_KEYPOINTS).to(device)\n",
    "print(model)\n",
    "\n",
    "# Criterion components\n",
    "reg_crit = nn.SmoothL1Loss()\n",
    "\n",
    "cosine = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "COSINE_WEIGHT = 0.1  # small auxiliary term\n",
    "\n",
    "\n",
    "def combined_loss(pred, target):\n",
    "    \"\"\"\n",
    "    pred, target: (B, 2*K), coordinates normalized in [0,1].\n",
    "    \"\"\"\n",
    "    reg_loss = reg_crit(pred, target)\n",
    "\n",
    "    # cosine similarity: we want vectors to point in same direction\n",
    "    pred_norm = pred - pred.mean(dim=1, keepdim=True)\n",
    "    tgt_norm  = target - target.mean(dim=1, keepdim=True)\n",
    "\n",
    "    cos_sim = cosine(pred_norm, tgt_norm)  # (B,)\n",
    "    cos_loss = (1.0 - cos_sim).mean()\n",
    "\n",
    "    return reg_loss + COSINE_WEIGHT * cos_loss, reg_loss.item(), cos_loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "074a55ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer & scheduler ready.\n"
     ]
    }
   ],
   "source": [
    "# BLOCK H: optimizer and scheduler\n",
    "\n",
    "BASE_LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "MAX_EPOCHS = 120\n",
    "PATIENCE = 15   # early stopping patience\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=MAX_EPOCHS,\n",
    "    eta_min=1e-5\n",
    ")\n",
    "\n",
    "print(\"Optimizer & scheduler ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3faa19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/120 (lr=0.001000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|          | 0/52 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# BLOCK I: metrics and training loop with checkpointing & early stopping\n",
    "\n",
    "def compute_l2_dist_px(pred, target):\n",
    "    \"\"\"\n",
    "    pred, target: (B, 2*K) normalized in [0,1].\n",
    "    returns mean L2 distance per keypoint in pixels (IMG_SIZE).\n",
    "    \"\"\"\n",
    "    B = pred.shape[0]\n",
    "    pred_xy = pred.view(B, -1, 2) * IMG_SIZE\n",
    "    tgt_xy  = target.view(B, -1, 2) * IMG_SIZE\n",
    "\n",
    "    dists = torch.linalg.norm(pred_xy - tgt_xy, dim=-1)  # (B, K)\n",
    "    return dists.mean().item()\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_reg_loss = 0.0\n",
    "    total_cos_loss = 0.0\n",
    "    total_dist = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=\"Train\", leave=False)\n",
    "    for batch in pbar:\n",
    "        imgs = batch[\"image\"].to(device)\n",
    "        targets = batch[\"keypoints\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(imgs)\n",
    "\n",
    "        loss, reg_l, cos_l = combined_loss(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_reg_loss += reg_l\n",
    "        total_cos_loss += cos_l\n",
    "        total_dist += compute_l2_dist_px(preds.detach(), targets.detach())\n",
    "        n_batches += 1\n",
    "\n",
    "        pbar.set_postfix(loss=total_loss/n_batches, dist=total_dist/n_batches)\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / n_batches,\n",
    "        \"reg_loss\": total_reg_loss / n_batches,\n",
    "        \"cos_loss\": total_cos_loss / n_batches,\n",
    "        \"dist_px\": total_dist / n_batches\n",
    "    }\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_one_epoch(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_reg_loss = 0.0\n",
    "    total_cos_loss = 0.0\n",
    "    total_dist = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=\"Val\", leave=False)\n",
    "    for batch in pbar:\n",
    "        imgs = batch[\"image\"].to(device)\n",
    "        targets = batch[\"keypoints\"].to(device)\n",
    "\n",
    "        preds = model(imgs)\n",
    "        loss, reg_l, cos_l = combined_loss(preds, targets)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_reg_loss += reg_l\n",
    "        total_cos_loss += cos_l\n",
    "        total_dist += compute_l2_dist_px(preds, targets)\n",
    "        n_batches += 1\n",
    "\n",
    "        pbar.set_postfix(loss=total_loss/n_batches, dist=total_dist/n_batches)\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / n_batches,\n",
    "        \"reg_loss\": total_reg_loss / n_batches,\n",
    "        \"cos_loss\": total_cos_loss / n_batches,\n",
    "        \"dist_px\": total_dist / n_batches\n",
    "    }\n",
    "\n",
    "\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"train_dist\": [],\n",
    "    \"val_dist\": [],\n",
    "    \"train_reg_loss\": [],\n",
    "    \"val_reg_loss\": [],\n",
    "    \"train_cos_loss\": [],\n",
    "    \"val_cos_loss\": []\n",
    "}\n",
    "\n",
    "CHECKPOINT_DIR = DATA_ROOT / \"checkpoints_resnet_yolo\"\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(1, MAX_EPOCHS + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{MAX_EPOCHS} (lr={optimizer.param_groups[0]['lr']:.6f})\")\n",
    "\n",
    "    train_metrics = train_one_epoch(model, train_loader, optimizer, device)\n",
    "    val_metrics   = eval_one_epoch(model, val_loader, device)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    history[\"train_loss\"].append(train_metrics[\"loss\"])\n",
    "    history[\"val_loss\"].append(val_metrics[\"loss\"])\n",
    "    history[\"train_dist\"].append(train_metrics[\"dist_px\"])\n",
    "    history[\"val_dist\"].append(val_metrics[\"dist_px\"])\n",
    "    history[\"train_reg_loss\"].append(train_metrics[\"reg_loss\"])\n",
    "    history[\"val_reg_loss\"].append(val_metrics[\"reg_loss\"])\n",
    "    history[\"train_cos_loss\"].append(train_metrics[\"cos_loss\"])\n",
    "    history[\"val_cos_loss\"].append(val_metrics[\"cos_loss\"])\n",
    "\n",
    "    print(\n",
    "        f\"Train - loss: {train_metrics['loss']:.4f} | \"\n",
    "        f\"reg: {train_metrics['reg_loss']:.4f} | \"\n",
    "        f\"cos: {train_metrics['cos_loss']:.4f} | \"\n",
    "        f\"L2 dist: {train_metrics['dist_px']:.2f} px\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Val   - loss: {val_metrics['loss']:.4f} | \"\n",
    "        f\"reg: {val_metrics['reg_loss']:.4f} | \"\n",
    "        f\"cos: {val_metrics['cos_loss']:.4f} | \"\n",
    "        f\"L2 dist: {val_metrics['dist_px']:.2f} px\"\n",
    "    )\n",
    "\n",
    "    # Save epoch checkpoint\n",
    "    ckpt_path = CHECKPOINT_DIR / f\"epoch_{epoch:03d}.pth\"\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"scheduler_state\": scheduler.state_dict(),\n",
    "            \"history\": history,\n",
    "        },\n",
    "        ckpt_path\n",
    "    )\n",
    "\n",
    "    # Early stopping logic\n",
    "    if val_metrics[\"loss\"] < best_val_loss - 1e-4:\n",
    "        best_val_loss = val_metrics[\"loss\"]\n",
    "        best_ckpt_path = CHECKPOINT_DIR / \"best_model.pth\"\n",
    "        torch.save(model.state_dict(), best_ckpt_path)\n",
    "        print(f\"‚úÖ New best model (val loss {best_val_loss:.4f}) saved to {best_ckpt_path}\")\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"No improvement for {epochs_no_improve} epoch(s).\")\n",
    "\n",
    "    if epochs_no_improve >= PATIENCE:\n",
    "        print(f\"‚èπ Early stopping triggered (patience = {PATIENCE}).\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7435f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Training Started...\n",
      "\n",
      "üîµ Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|          | 0/33 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# BLOCK F ‚Äî UPDATED TRAINING LOOP FOR FAST DATASET (NO SEGMENTATION IN LOOP)\n",
    "\n",
    "def compute_l2_dist(pred, target):\n",
    "    \"\"\"\n",
    "    pred, target: (B, 52) ‚Üí flattened (x1,y1,...)\n",
    "    returns mean L2 dist per keypoint across the batch.\n",
    "    \"\"\"\n",
    "    B = pred.shape[0]\n",
    "    pred_xy = pred.view(B, -1, 2)   # (B, 26, 2)\n",
    "    tgt_xy  = target.view(B, -1, 2)\n",
    "\n",
    "    dists = torch.linalg.norm(pred_xy - tgt_xy, dim=-1)  # (B, 26)\n",
    "    return dists.mean().item()\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_dist = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for imgs, targets in tqdm(loader, desc=\"Train\", leave=False):\n",
    "        imgs    = imgs.float().to(device)      # (B, 3, 512, 512)\n",
    "        targets = targets.float().to(device)   # (B, 52)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(imgs)\n",
    "        loss = crit(preds, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_dist += compute_l2_dist(preds.detach().cpu(), targets.detach().cpu())\n",
    "        n_batches += 1\n",
    "\n",
    "    return total_loss / n_batches, total_dist / n_batches\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_one_epoch(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_dist = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for imgs, targets in tqdm(loader, desc=\"Val\", leave=False):\n",
    "        imgs    = imgs.float().to(device)\n",
    "        targets = targets.float().to(device)\n",
    "\n",
    "        preds = model(imgs)\n",
    "        loss = crit(preds, targets)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_dist += compute_l2_dist(preds.detach().cpu(), targets.detach().cpu())\n",
    "        n_batches += 1\n",
    "\n",
    "    return total_loss / n_batches, total_dist / n_batches\n",
    "\n",
    "\n",
    "# ---------- MAIN TRAINING LOOP ----------\n",
    "num_epochs = 30\n",
    "best_val_loss = float(\"inf\")\n",
    "history = {\"train_loss\": [], \"val_loss\": [], \"train_dist\": [], \"val_dist\": []}\n",
    "\n",
    "CHECKPOINT_DIR = DATA_ROOT / \"checkpoints\"\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"üöÄ Training Started...\")\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(f\"\\nüîµ Epoch {epoch}/{num_epochs}\")\n",
    "\n",
    "    train_loss, train_dist = train_one_epoch(model, train_loader, optimizer, device)\n",
    "    val_loss,   val_dist   = eval_one_epoch(model, val_loader, device)\n",
    "\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"train_dist\"].append(train_dist)\n",
    "    history[\"val_dist\"].append(val_dist)\n",
    "\n",
    "    print(f\"Train ‚Üí loss: {train_loss:.4f}, L2 dist: {train_dist:.2f}px\")\n",
    "    print(f\"Val   ‚Üí loss: {val_loss:.4f}, L2 dist: {val_dist:.2f}px\")\n",
    "\n",
    "    # save per-epoch checkpoint\n",
    "    ckpt_path = CHECKPOINT_DIR / f\"epoch_{epoch:03d}.pth\"\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"history\": history,\n",
    "    }, ckpt_path)\n",
    "\n",
    "    # best model tracking\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_ckpt = CHECKPOINT_DIR / \"best_model.pth\"\n",
    "        torch.save(model.state_dict(), best_ckpt)\n",
    "        print(f\"‚úÖ Best model updated ‚Üí {best_ckpt}\")\n",
    "\n",
    "print(\"\\nüéâ Training Complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eea456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b244c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics curves\n",
    "\n",
    "epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, history[\"train_loss\"], label=\"Train loss\")\n",
    "plt.plot(epochs, history[\"val_loss\"], label=\"Val loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"SmoothL1 loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, history[\"train_dist\"], label=\"Train L2 dist\")\n",
    "plt.plot(epochs, history[\"val_dist\"], label=\"Val L2 dist\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Mean L2 distance (px)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc37bad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:32<00:00,  1.01it/s]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:04<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.071767 | Val Loss: 0.025882\n",
      "‚úî Saved Best Model\n",
      "\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:20<00:00,  1.57it/s]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:03<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.005927 | Val Loss: 0.004911\n",
      "‚úî Saved Best Model\n",
      "\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:20<00:00,  1.61it/s]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:03<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.003482 | Val Loss: 0.002139\n",
      "‚úî Saved Best Model\n",
      "\n",
      "Epoch 4/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:20<00:00,  1.64it/s]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:02<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.002995 | Val Loss: 0.002258\n",
      "\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:20<00:00,  1.61it/s]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:02<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.002508 | Val Loss: 0.001810\n",
      "‚úî Saved Best Model\n",
      "\n",
      "Epoch 6/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:19<00:00,  1.67it/s]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:02<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.002799 | Val Loss: 0.003610\n",
      "\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:19<00:00,  1.68it/s]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:02<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.004267 | Val Loss: 0.001447\n",
      "‚úî Saved Best Model\n",
      "\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:19<00:00,  1.74it/s]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:02<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.001663 | Val Loss: 0.001266\n",
      "‚úî Saved Best Model\n",
      "\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:19<00:00,  1.73it/s]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:02<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.002088 | Val Loss: 0.001693\n",
      "\n",
      "Epoch 10/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:18<00:00,  1.76it/s]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.002363 | Val Loss: 0.001901\n",
      "\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:11<00:00,  2.83it/s]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.001508 | Val Loss: 0.000714\n",
      "‚úî Saved Best Model\n",
      "\n",
      "Epoch 12/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:11<00:00,  2.82it/s]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.001022 | Val Loss: 0.000559\n",
      "‚úî Saved Best Model\n",
      "\n",
      "Epoch 13/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:11<00:00,  2.82it/s]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.000840 | Val Loss: 0.000470\n",
      "‚úî Saved Best Model\n",
      "\n",
      "Epoch 14/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:11<00:00,  2.84it/s]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.001033 | Val Loss: 0.000524\n",
      "\n",
      "Epoch 15/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:11<00:00,  2.83it/s]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.000912 | Val Loss: 0.000635\n",
      "\n",
      "Epoch 16/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:11<00:00,  2.83it/s]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.000967 | Val Loss: 0.000462\n",
      "‚úî Saved Best Model\n",
      "\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:11<00:00,  2.80it/s]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.001071 | Val Loss: 0.000496\n",
      "\n",
      "Epoch 18/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:11<00:00,  2.81it/s]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.000877 | Val Loss: 0.000477\n",
      "\n",
      "Epoch 19/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:11<00:00,  2.82it/s]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.000831 | Val Loss: 0.000483\n",
      "\n",
      "Epoch 20/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:11<00:00,  2.82it/s]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.001067 | Val Loss: 0.000538\n",
      "\n",
      "Epoch 21/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:11<00:00,  2.82it/s]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.000827 | Val Loss: 0.000473\n",
      "\n",
      "Epoch 22/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:11<00:00,  2.81it/s]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.000816 | Val Loss: 0.000496\n",
      "\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:11<00:00,  2.81it/s]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.000788 | Val Loss: 0.000487\n",
      "\n",
      "Epoch 24/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:11<00:00,  2.86it/s]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.000812 | Val Loss: 0.000483\n",
      "\n",
      "Epoch 25/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:11<00:00,  2.85it/s]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.000778 | Val Loss: 0.000418\n",
      "‚úî Saved Best Model\n",
      "\n",
      "Epoch 26/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:11<00:00,  2.83it/s]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.000666 | Val Loss: 0.000455\n",
      "\n",
      "Epoch 27/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:11<00:00,  2.83it/s]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.000724 | Val Loss: 0.000451\n",
      "\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:12<00:00,  2.64it/s]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.000906 | Val Loss: 0.000418\n",
      "\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:12<00:00,  2.57it/s]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.000850 | Val Loss: 0.000438\n",
      "\n",
      "Epoch 30/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:11<00:00,  2.83it/s]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.000601 | Val Loss: 0.000430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# BLOCK G: visualize predictions vs ground truth\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize_predictions(model, dataset, num_samples=6):\n",
    "    model.eval()\n",
    "    idxs = np.random.choice(len(dataset), size=min(num_samples, len(dataset)), replace=False)\n",
    "\n",
    "    num_cols = 3\n",
    "    num_rows = int(np.ceil(len(idxs) / num_cols))\n",
    "\n",
    "    plt.figure(figsize=(5 * num_cols, 5 * num_rows))\n",
    "\n",
    "    for i, idx in enumerate(idxs):\n",
    "        item = dataset[idx]\n",
    "        img = item[\"image\"].unsqueeze(0).to(device)\n",
    "        gt_kpts = item[\"keypoints\"].numpy().reshape(-1, 2)\n",
    "\n",
    "        pred = model(img).cpu().numpy().reshape(-1, 2)\n",
    "\n",
    "        # de-normalize image\n",
    "        img_np = item[\"image\"].numpy()\n",
    "        img_np = np.transpose(img_np, (1, 2, 0))\n",
    "        img_np = img_np * train_dataset.std + train_dataset.mean\n",
    "        img_np = np.clip(img_np, 0, 1)\n",
    "\n",
    "        plt.subplot(num_rows, num_cols, i + 1)\n",
    "        plt.imshow(img_np)\n",
    "        plt.scatter(gt_kpts[:, 0], gt_kpts[:, 1], s=60, c=\"lime\", label=\"GT\")\n",
    "        plt.scatter(pred[:, 0], pred[:, 1], s=60, c=\"red\", marker=\"x\", label=\"Pred\")\n",
    "        plt.title(f\"idx={idx}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    # Only one legend for all subplots\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    plt.figlegend(handles, labels, loc=\"upper right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "visualize_predictions(model, val_dataset, num_samples=9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d313cf8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
